# MCP Server Configuration Guide

This guide explains all configuration options available for the Ollama Chat MCP Server.

## Configuration Methods

The MCP server can be configured in two ways:

1. **Environment Variables** - Set in your MCP client configuration (e.g., Claude Desktop's config file)
2. **Command Line Arguments** - Pass directly when starting the server (for advanced use cases)

Configuration values take precedence in this order:
1. Tool-specific parameters (passed when calling a tool)
2. MCP server configuration (environment variables)
3. System environment variables
4. Default values

## Available Configuration Options

### Provider Selection

**Environment Variable:** `MCP_PROVIDER`

Controls which AI provider to use.

**Valid Values:**
- `auto` (default) - Automatically detect provider based on available credentials
- `azure` - Force Azure OpenAI (requires Azure credentials)
- `openai` - Force OpenAI (requires OpenAI API key or local OpenAI server)
- `ollama` - Force Ollama (requires Ollama running locally)

**Priority (auto mode):**
1. Azure OpenAI (if `AZURE_OPENAI_API_KEY` + `AZURE_OPENAI_ENDPOINT` + deployment are set)
2. OpenAI (if `OPENAI_API_KEY` is set)
3. Ollama (default fallback)

### Model Configuration

#### Azure OpenAI Deployment Name

**Environment Variable:** `MCP_AZURE_DEPLOYMENT`

Specifies the Azure OpenAI deployment name to use. Takes precedence over the `AZURE_OPENAI_DEPLOYMENT` environment variable.

**Example:** `gpt-4`, `gpt-35-turbo`, `my-custom-deployment`

**Required when:** Using Azure OpenAI provider

#### Ollama Model

**Environment Variable:** `MCP_OLLAMA_MODEL`

Specifies the Ollama model to use for all operations.

**Example:** `qwen3:4b`, `llama3:8b`, `mistral:7b`

**Note:** If the model name doesn't contain a `:`, it will automatically append `:latest`

#### OpenAI Model

**Environment Variable:** `MCP_OPENAI_MODEL`

Specifies the OpenAI model to use for all operations.

**Example:** `gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo-preview`

#### Embeddings Model

**Environment Variable:** `MCP_EMBEDDINGS_MODEL`

Specifies the Ollama embeddings model to use for vector database operations (web search).

**Default:** `nomic-embed-text`

**Example:** `nomic-embed-text`, `mxbai-embed-large`, `all-minilm`

**⚠️ IMPORTANT:** Once you start using a ChromaDB database with a specific embeddings model, you **cannot change the model** without creating a new database. ChromaDB databases are tied to the embedding model used to create them. If you need to change models, you must either:
1. Delete the existing ChromaDB database directory and start fresh, or
2. Configure a different `MCP_CHROMADB_PATH` for the new model

**Note:** This setting applies to all AI providers (Ollama, Azure OpenAI, OpenAI) as embeddings are always generated by Ollama.

### ChromaDB Configuration

#### ChromaDB Path

**Environment Variable:** `MCP_CHROMADB_PATH`

Specifies the directory where the ChromaDB vector database is stored. This database is used by the `web_search` tool to store and search indexed web content.

**Default Locations (OS-specific):**
- **Windows:** `%LOCALAPPDATA%\ollama-chat\chromadb` (e.g., `C:\Users\YourName\AppData\Local\ollama-chat\chromadb`)
- **macOS:** `~/Library/Application Support/ollama-chat/chromadb`
- **Linux:** `~/.local/share/ollama-chat/chromadb` (respects `XDG_DATA_HOME` if set)

**Example:** 
- Windows: `C:\Users\YourName\Documents\ollama-chat-db`
- macOS/Linux: `/home/yourname/data/chromadb`

**Note:** The directory will be created automatically if it doesn't exist. The database persists across MCP server sessions, allowing you to build up a knowledge base over time.

### Tools Configuration

**Environment Variable:** `MCP_ALLOWED_TOOLS`

Restricts which ollama_chat.py tools can be used. If not set, all tools are available.

**Format:** Comma-separated list of tool names

**Example:** `web_search,file_reader,python_executor`

**Special Note:** The `list_available_tools` MCP tool is always available regardless of this setting.

To see all available tools, use the `list_available_tools` tool or run:
```bash
python ollama_chat.py --list-tools
```

## Configuration Examples

### Example 1: Auto-detect with Ollama Model Preference

```json
{
  "mcpServers": {
    "ollama-chat": {
      "command": "node",
      "args": ["c:\\dev\\perso\\ollama-chat\\mcp-server\\index.js"],
      "env": {
        "MCP_PROVIDER": "auto",
        "MCP_OLLAMA_MODEL": "qwen3:4b",
        "MCP_EMBEDDINGS_MODEL": "nomic-embed-text",
        "MCP_CHROMADB_PATH": "C:\\Users\\YourName\\Documents\\ollama-chat-db"
      }
    }
  }
}
```

This configuration:
- Auto-detects the provider (will use Ollama if no cloud credentials are found)
- Uses `qwen3:4b` model for chat and web search
- Uses `nomic-embed-text` for embeddings (web search indexing)
- Stores ChromaDB database in a custom location (optional - will use OS default if not specified)

### Example 2: Azure OpenAI with Custom Deployment

```json
{
  "mcpServers": {
    "ollama-chat": {
      "command": "node",
      "args": ["c:\\dev\\perso\\ollama-chat\\mcp-server\\index.js"],
      "env": {
        "MCP_PROVIDER": "azure",
        "AZURE_OPENAI_API_KEY": "your-api-key",
        "AZURE_OPENAI_ENDPOINT": "https://your-resource.openai.azure.com/",
        "MCP_AZURE_DEPLOYMENT": "gpt-4",
        "MCP_ALLOWED_TOOLS": "web_search,file_reader"
      }
    }
  }
}
```

This configuration:
- Forces Azure OpenAI provider
- Uses the `gpt-4` deployment
- Restricts tools to only `web_search` and `file_reader`

### Example 3: OpenAI with Specific Model

```json
{
  "mcpServers": {
    "ollama-chat": {
      "command": "node",
      "args": ["c:\\dev\\perso\\ollama-chat\\mcp-server\\index.js"],
      "env": {
        "MCP_PROVIDER": "openai",
        "OPENAI_API_KEY": "sk-your-api-key",
        "MCP_OPENAI_MODEL": "gpt-4-turbo-preview"
      }
    }
  }
}
```

This configuration:
- Forces OpenAI provider
- Uses `gpt-4-turbo-preview` model for all operations

### Example 4: Ollama with Multiple Restrictions

```json
{
  "mcpServers": {
    "ollama-chat": {
      "command": "node",
      "args": ["c:\\dev\\perso\\ollama-chat\\mcp-server\\index.js"],
      "env": {
        "MCP_PROVIDER": "ollama",
        "MCP_OLLAMA_MODEL": "llama3:8b",
        "MCP_EMBEDDINGS_MODEL": "mxbai-embed-large",
        "MCP_ALLOWED_TOOLS": "web_search"
      }
    }
  }
}
```

This configuration:
- Forces Ollama provider
- Uses `llama3:8b` for chat
- Uses `mxbai-embed-large` for embeddings
- Only allows the `web_search` tool

## Tool-Level Model Override

Even with server-level configuration, you can override the model for individual tool calls:

```json
{
  "tool": "chat",
  "arguments": {
    "message": "What is quantum computing?",
    "model": "qwen3:14b"
  }
}
```

This will use `qwen3:14b` for this specific call, regardless of the `MCP_OLLAMA_MODEL` setting.

## Required Environment Variables by Provider

### Azure OpenAI
- `AZURE_OPENAI_API_KEY` - Your Azure OpenAI API key
- `AZURE_OPENAI_ENDPOINT` - Your Azure OpenAI endpoint URL
- `MCP_AZURE_DEPLOYMENT` or `AZURE_OPENAI_DEPLOYMENT` - Deployment name

### OpenAI
- `OPENAI_API_KEY` - Your OpenAI API key (optional if using local server)

### Ollama
- No environment variables required
- Ollama must be running locally (default: `http://localhost:11434`)

## Troubleshooting

### How do I know which provider is being used?

Check the server logs (stderr). The server logs its provider selection:
- `[Info] Using Azure OpenAI (auto-detected) - Deployment: gpt-4`
- `[Info] Using OpenAI (explicitly configured)`
- `[Info] Using Ollama (default - no cloud API keys detected)`

### What tools are available?

Use the `list_available_tools` MCP tool to see all available ollama_chat.py tools:
- Built-in tools (web search, file operations, etc.)
- Plugin tools (if you have plugins installed)

### Can I use different models for web_search vs chat?

Yes! Just pass the `model` parameter when calling each tool:
- For web_search: `{"query": "...", "model": "qwen3:4b"}`
- For chat: `{"message": "...", "model": "llama3:8b"}`

### How do I see what's happening during a web search?

Use the `show_intermediate` parameter when calling the `web_search` tool:
```json
{
  "query": "What is Python?",
  "show_intermediate": true
}
```

This will display:
1. Search results from DuckDuckGo (titles, URLs, snippets)
2. URLs being crawled
3. Content previews from each crawled page
4. Vector database retrieval results

This is useful for:
- Understanding why certain answers were generated
- Debugging search quality issues
- Verifying content is being crawled correctly
- Transparency in AI reasoning

### My Azure deployment isn't being recognized

Make sure:
1. `AZURE_OPENAI_API_KEY` is set
2. `AZURE_OPENAI_ENDPOINT` is set
3. Either `MCP_AZURE_DEPLOYMENT` or `AZURE_OPENAI_DEPLOYMENT` is set
4. The deployment name matches exactly what's in Azure Portal

## Advanced: Direct Python Script Access

The MCP server ultimately calls `ollama_chat.py`. All configuration options map to command-line arguments:

| MCP Config | Python Argument |
|------------|----------------|
| `MCP_AZURE_DEPLOYMENT` | `--model=<deployment>` |
| `MCP_OLLAMA_MODEL` | `--model=<model>` |
| `MCP_OPENAI_MODEL` | `--model=<model>` |
| `MCP_EMBEDDINGS_MODEL` | `--embeddings-model=<model>` |
| `MCP_ALLOWED_TOOLS` | `--tools=<tool1,tool2>` |
| `MCP_PROVIDER=azure` | `--use-azure-openai` |
| `MCP_PROVIDER=openai` | `--use-openai` |

You can run `python ollama_chat.py --help` to see all available options.
